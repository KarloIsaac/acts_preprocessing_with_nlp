{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import settings\n",
    "import shutil\n",
    "import numpy as np\n",
    "import re\n",
    "from IPython.display import clear_output\n",
    "import docx\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obtaining the corpus\n",
    "\n",
    "Up until the day 2019-02-26, we have a corpus of almost 15500 files related to the inspection acts. Most of the have the .doc or .docx extension. For our purpose, these are the files we are interested on. Every file should be named exclusively with the inspection act number. This number is built with the following regex pattern:\n",
    "\n",
    "    \"\\d{2}-[A-Z]{2}-\\d{4}-\\d{5}-[A-Z]{2}\"\n",
    "    \n",
    "While all the files contain this identifier in their name, they may also contain \"noise\" information; therefore, the first step is to normalize the name of the files. We may also have  other type of files  as .xlsx, .pdf or image files. The corpus will only have files with extension .doc or . docx, named exclusively as the inspection act number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def import_corpus_files():\n",
    "    confirmation = input(\"This is a lenghty process that should be called\"\\\n",
    "                        \" just once. Are you sure you want to execute it?\"\\\n",
    "                        \" [Y/N]\")\n",
    "    if confirmation != \"Y\": \n",
    "        print(\"aborting...\")\n",
    "        clear_output()\n",
    "        return\n",
    "    destination_directory = Path(settings.CORPUS_DIRECTORY_ROUTE)\n",
    "    source_directory = Path(settings.INSPECTION_ACTS_ROUTE)\n",
    "    pattern = re.compile(r\".*(\\d{2}-[A-Z]{2}-\\d{4}-\\d{5}-[A-Z]{2})\"\\\n",
    "                         \".*(\\.(?:doc|docx))$\")\n",
    "    for file in source_directory.iterdir():\n",
    "        match = pattern.match(file.name)\n",
    "        if match:        \n",
    "            shutil.copyfile(file, destination_directory.absolute()\\\n",
    "                    / Path(match.group(1) + match.group(2))) \n",
    "import_corpus_files()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting the corpus to a readable format\n",
    "\n",
    "In order to read the files, we are going to use the [python-docx](https://github.com/python-openxml/python-docx) library. Now, we face the problem that this library works exclusively with .docx files.\n",
    "\n",
    "We need to change all the .doc files in the corpus to .docx files. To achieve this, we are going to use the script ./from_doc_to_docx.vbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_files_extensions():\n",
    "    source_directory = Path(settings.INSPECTION_ACTS_ROUTE)\n",
    "    script_address = str(Path(\"./from_doc_to_docx.vbs\").absolute())\n",
    "    for file in source_directory.iterdir():        \n",
    "        if file.suffix == \".doc\":\n",
    "            subprocess.run([\"wscript.exe\", script_address, \n",
    "                            str(file.absolute())])\n",
    "            file.unlink()\n",
    "change_files_extensions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After executing this piece of code, we have a corpus of 15 203 documents; almost 7 GB of data.\n",
    "\n",
    "## Examining the corpus\n",
    "\n",
    "Now, we have some questions: What words are more frequent? What is their distribution?. It'd be a good idea to start exploring the corpus. Since it is toot big, we are going to take a sample of 800 documents (around 5% of all the corpus) chosen randomly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\projects\\acts_preprocessing_with_nlp\\data\\corpus\\c.txt\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "\n",
    "np.random.uniform\n",
    "random_corpus = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WindowsPath('data/corpus/13-MF-3309-05093-DD.docx'),\n",
       " WindowsPath('data/corpus/14-AF-3301-03251-CV.doc'),\n",
       " WindowsPath('data/corpus/14-AF-3301-03252-CV.doc'),\n",
       " WindowsPath('data/corpus/14-AF-3301-03253-CV.doc'),\n",
       " WindowsPath('data/corpus/14-AF-3301-04554-IV.doc')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "corpus_directory = Path(\"./data/corpus\")\n",
    "files_collection = [file for file in corpus_directory.iterdir()]\n",
    "files_collection[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xae in position 14: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-1e4bf0301220>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfiles_collection\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"UTF-8\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\lib\\pathlib.py\u001b[0m in \u001b[0;36mread_text\u001b[1;34m(self, encoding, errors)\u001b[0m\n\u001b[0;32m   1195\u001b[0m         \"\"\"\n\u001b[0;32m   1196\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'r'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1197\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1199\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mwrite_bytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\lib\\codecs.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m    319\u001b[0m         \u001b[1;31m# decode input (taking the buffer into account)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    320\u001b[0m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 321\u001b[1;33m         \u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconsumed\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_buffer_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    322\u001b[0m         \u001b[1;31m# keep undecoded input until the next call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    323\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mconsumed\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xae in position 14: invalid start byte"
     ]
    }
   ],
   "source": [
    "files_collection[0].read_text(encoding=\"UTF-8\")[0:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(file.iterdir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "227px",
    "left": "0px",
    "top": "110px",
    "width": "342px"
   },
   "toc_section_display": false,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
